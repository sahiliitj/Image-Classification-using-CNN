# -*- coding: utf-8 -*-
"""Deep Learning A1 M21MA210

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xfeQahQMmETtRfa4x7qtIPXXnB25EUpi

#  Deep Learning (CSL7590): Assigment-1 (Designing a CNN )

# Importing the libraries required
"""

pip install idx2numpy

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
import numpy as np
import idx2numpy as im

data_a= '/content/drive/MyDrive/Deep Learning 2024/train-images.idx3-ubyte'
data_b = '/content/drive/MyDrive/Deep Learning 2024/train-labels.idx1-ubyte'
data_c= '/content/drive/MyDrive/Deep Learning 2024/t10k-images.idx3-ubyte'
data_d = '/content/drive/MyDrive/Deep Learning 2024/t10k-labels.idx1-ubyte'
train_x = im.convert_from_file(data_a)
train_y = im.convert_from_file(data_b)
test_x = im.convert_from_file(data_c)
test_y = im.convert_from_file(data_d)
# Printing the shapes of our dataset viz training and test dataset
print(f"{train_x.shape} is the shape of training data")
print(f"{train_y.shape} is the shape of training data labels")
print(f"{test_x.shape} is the shape of test data")
print(f"{test_y.shape} is the shape of test data labels")

"""# Exploratory Data Analysis of the given dataset"""

# Visualising the dataset to see how the images look
from google.colab.patches import cv2_imshow
import cv2
cv2_imshow(cv2.resize(train_x[9896], (256, 256)))

# Checking for the sample images present in our dataset
import matplotlib.pyplot as plt
print("Sample images from the given dataset are as: ")
def display(data):
    f,ax = plt.subplots(1,8, figsize=(15,15))
    for i in range(8):
        ax[i].imshow(data[i], cmap='gray')
        ax[i].set_title(f"{i+1}")
        ax[i].axis('off')
    plt.show()
display(train_x)

# Checking for distribution of the intensity of pixels in the images
import matplotlib.pyplot as plt
plt.hist(train_x.flatten(),bins=256,color='red')
plt.title('Pixels Intensity Distribution among images in the given dataset')
plt.xlabel('Value of Pixel')
plt.ylabel('Intensity of pixels')
plt.grid(axis='both', linestyle='--')
plt.show()

# Determining the label occurences in the dataset
import seaborn as sns
lab_num= np.bincount(train_y)
labels = np.arange(len(lab_num))
sns.set(style="whitegrid")
plt.bar(labels, lab_num, color=sns.color_palette('Set2', len(lab_num)))
plt.title('Label Distribution')
plt.xlabel('Label assigned to the image in the dataset')
plt.ylabel('Count of the labels')
plt.xticks(labels)
plt.show()

"""# Construction of the CNN Architecture"""

# Training data size
print("The training data size is: " ,train_x.size)
print("The training data label size is: " ,train_y.size)

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as func

class CNNArch(nn.Module):
  def __init__(self, lab):
    nn.Module.__init__(self)
    con1_par = {'in_channels':1,'out_channels':16,'kernel_size':7,'stride':1,'padding':3}
    self.conv1 = nn.Conv2d(**con1_par)
    con2_par = {'in_channels':16,'out_channels':8,'kernel_size':5,'stride':1,'padding':2}
    self.conv2 = nn.Conv2d(**con2_par)
    con3_par = {'in_channels':8,'out_channels':4,'kernel_size':3,'stride':1,'padding':1}
    self.conv3 = nn.Conv2d(**con3_par)
    full_layer_par = {'in_features':4*3*3,'out_features':lab}
    self.fc_layer = nn.Linear(**full_layer_par)
    #self.dp_flayer= nn.Dropout(p=0.2)
    self.softmax= nn.Softmax(dim=1)
    self.mpool = nn.MaxPool2d(2,2)
    self.apool= nn.AvgPool2d(2,2)
  def forward(self,x):
    x=self.conv1(x)
    x=func.relu(x)
    x=self.mpool(x)
    x=self.conv2(x)
    x=func.relu(x)
    x=self.mpool(x)
    x=self.conv3(x)
    x=func.relu(x)
    x=self.apool(x)
    x=torch.flatten(x,start_dim=1)
    x=self.fc_layer(x)
    #x=self.dp_flayer(x)
    x=self.softmax(x)
    return x
print(CNNArch)

"""# Training the model"""

#Convert data to pytorch tensors
import torch
def convert(x,y):
  x = torch.tensor(x,dtype=torch.float32).unsqueeze(1)
  y = torch.tensor(y,dtype=torch.long)
  return x,y

# Loading the data so that we can able to train the model by passing our training data into it
import torch
from torch.utils.data import TensorDataset, DataLoader
def data_loading(train_x, train_y, batch_size=32, shuffle=True):
    train_data = TensorDataset(train_x, train_y)
    train_load = DataLoader(train_data, batch_size=32, shuffle=True)
    return train_load

from torch.optim import lr_scheduler
def criteria(cnn_mod):
  loss_cr = nn.CrossEntropyLoss()
  opt = optim.Adam(cnn_mod.parameters(), lr=0.001, weight_decay= 1e-4)
  sch = lr_scheduler.StepLR(opt, step_size=5, gamma=0.9)
  return loss_cr,opt,sch

cnn_mod =CNNArch(10)

train_x, train_y = convert(train_x,train_y)

training_data = data_loading(train_x,train_y)

loss_cr, opt,sch = criteria(cnn_mod)

def model_training(cnn_mod,training_data,opt,loss_cr,epochs,sch):
  loss_hist=[]
  acc_hist =[]
  print('-' * 63 )
  print("Training the model and noting the loss and accuracy each epoch")
  print('-' * 63 )
  for i in range(epochs):
    cnn_mod.train()
    ep_loss, pred, samples=0.0,0,0
    for j,k in training_data:
      opt.zero_grad()
      out= cnn_mod(j)
      loss = loss_cr(out,k)
      loss.backward()
      opt.step()
      ep_loss+= loss.item()
      _, predicts = torch.max(out,1)
      pred += (predicts == k).sum().item()
      samples += k.size(0)
    avg_loss = ep_loss/ len(training_data)
    acc = (pred/samples)*100
    print(f" This is Epoch {i+1} where, Loss is: {avg_loss:.4f} & Accuracy is: {acc:.4f}%")
    loss_hist.append(avg_loss)
    acc_hist.append(acc)
  return loss_hist, acc_hist

# Training the model
ep = 10
loss_hist, acc_hist = model_training(cnn_mod,training_data,opt,loss_cr,ep,sch)

# Plotting the training loss curve
import matplotlib.pyplot as plt
def loss_curve(history):
  plt.plot(history, label=' Loss ', color ='red')
  plt.title('Loss during the training')
  plt.xlabel('Number of epochs')
  plt.ylabel('Values of the loss occured during the training')
  return plt.show()
# Plotting the Accuracy curve, achieved during the training
def acc_curve(history):
  plt.plot(history, label=' Loss ', color ='red')
  plt.title('Achieved accuracy during the training')
  plt.xlabel('Number of epochs')
  plt.ylabel('Accuracy of the model during the training')
  return plt.show()

# Loss curve Plot for training set
loss_curve(loss_hist)

# Loss curve Plot for training set
acc_curve(acc_hist)

# Testing the model on test dataset
import torch
def model_test(CNN, test_set):
    CNN.eval()
    labs = []
    preds = []
    with torch.no_grad():
        for i,t in test_set:
            out = CNN(i)
            _, given = torch.max(out, 1)
            labs.extend(t.numpy())
            preds.extend(given.numpy())
    acc = sum([1 for i, j in zip(labs, preds) if i == j]) / len(labs)
    return acc, labs, preds

test_x, test_y = convert(test_x,test_y)

testing_data = data_loading(test_x,test_y)

# Prediction of accuracy of the model on the test dataset
accuracy, true_lab, pred_lab = model_test(cnn_mod, testing_data)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Calculate the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(true_lab,pred_lab)
# Create a heatmap using seaborn
sns.set(font_scale=0.9)
sns.heatmap(cm, annot=True, fmt='d',cmap = 'YlOrBr',cbar='False', xticklabels=range(10), yticklabels=range(10))
plt.xlabel('Prediction by the model')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for the test dataset')
plt.show()

from sklearn.metrics import classification_report
report = classification_report(true_lab, pred_lab)
print("Classification Report:\n", report)

t_pars, nt_pars=0,0
for i in cnn_mod.parameters():
  if i.requires_grad:
    t_pars += i.numel()
  else:
    nt_pars+= i.numel()
print(f"Total number of parameters are : { t_pars + nt_pars }")
print(f"Total Trainable parameters in the model are: {t_pars}")
print(f" Total Non-Trainable parameters in the model are: {nt_pars}")

for i,j in cnn_mod.named_parameters():
    print(f"Parameter name: {i}, Size: {j.size()}")

path_to_file ='/content/drive/MyDrive/Deep Learning 2024/Parameters_CNN.txt'
file = open(path_to_file,'w')
file.write("\n \n  EXPERIMENT 1 PARAMETERS \n \n ")
for i,j in cnn_mod.named_parameters():
        file.write(f"Parameter name: {i}, Size: {j.size()}\n")
        file.write(f"Values: {j}\n\n")
file.close()

"""# Experiment 2

"""

#Mapping the given lables to the new classification classes
map_label = { 0:0,
              1:1,
              2:2,
              3:2,
              4:3,
              5:2,
              6:0,
              7:1,
              8:2,
              9:3}

# Fetching the data again for new model
new_train_x = im.convert_from_file(data_a)
new_train_y = im.convert_from_file(data_b)
new_test_x = im.convert_from_file(data_c)
new_test_y = im.convert_from_file(data_d)

# Modifying the existing dataset to new dataset
import torch
def data_modified(data_x, data_y, class_mapping):
    new_data = []
    new_labels = []
    for x, y in zip(data_x, data_y):
        new_label = class_mapping[y.item()]
        new_data.append(torch.tensor(x))  # Convert to tensor
        new_labels.append(new_label)
    return torch.stack(new_data), torch.tensor(new_labels,dtype=torch.long)

# Construction of new model for new mutli class classification
cnn_mod2 = CNNArch(4)
loss_cr2, opt2,sch2 = criteria(cnn_mod2)

new_train_x,new_train_y= data_modified(new_train_x,new_train_y,map_label)

new_train_x,new_train_y=convert(new_train_x,new_train_y)

new_trainset = data_loading(new_train_x,new_train_y)

# Training the model for new dataset
ep2 = 10
acc_hist2, loss_hist2 = model_training(cnn_mod2,new_trainset,opt2,loss_cr2,ep2,sch)

#Plotting the loss curve
loss_curve(loss_hist2)

# Plotting the accuracy curve
acc_curve(acc_hist2)

new_test_x,new_test_y= data_modified(new_test_x,new_test_y,map_label)

new_test_x,new_test_y=convert(new_test_x,new_test_y)

new_testset = data_loading(new_test_x,new_test_y)

# Testing the accuracy on new test dataset
accuracy2, tr_lab, pr_lab = model_test(cnn_mod2, new_testset)
print(f'Accuracy: {accuracy2 * 100:.2f}%')

# Calculate the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(tr_lab,pr_lab)
# Create a heatmap using seaborn
sns.set(font_scale=1.0)
sns.heatmap(cm, annot=True, fmt='d',cmap = 'YlOrBr',cbar='False', xticklabels=range(4), yticklabels=range(4))
plt.xlabel('Prediction by the model')
plt.ylabel('True Labels')
plt.title('Confusion Matrix for the test dataset')
plt.show()

from sklearn.metrics import classification_report
rep2 = classification_report(tr_lab, pr_lab)
print("Classification Report:\n", rep2)

t_pars2, nt_pars2=0,0
for i in cnn_mod2.parameters():
  if i.requires_grad:
    t_pars2 += i.numel()
  else:
    nt_pars2+= i.numel()
print(f"Total number of parameters are : { t_pars2 + nt_pars2 }")
print(f"Total Trainable parameters in the model are: {t_pars2}")
print(f" Total Non-Trainable parameters in the model are:: {nt_pars2}")

for i,j in cnn_mod2.named_parameters():
    print(f"Parameter name: {i}, Size: {j.size()}")

path_to_file ='/content/drive/MyDrive/Deep Learning 2024/Parameters_CNN.txt'
file = open(path_to_file,'a')
file.write("\n \n  EXPERIMENT 2 PARAMETERS \n \n ")
for i,j in cnn_mod2.named_parameters():
        file.write(f"Parameter name: {i}, Size: {j.size()}\n")
        file.write(f"Values: {j}\n\n")
file.close()